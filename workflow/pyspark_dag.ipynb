{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06e6358-79a6-4c8a-96c7-0063df608169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all modules\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from datetime import timedelta\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.providers.google.cloud.operators.dataproc import (\n",
    "    DataprocStartClusterOperator,\n",
    "    DataprocStopClusterOperator,\n",
    "    DataprocSubmitJobOperator,\n",
    ")\n",
    "\n",
    "\n",
    "# define the variables\n",
    "PROJECT_ID = \"GCP-DEMO-LTE\"\n",
    "REGION = \"us-east1\"\n",
    "CLUSTER_NAME = \"cluster-lte-demo\"\n",
    "COMPOSER_BUCKET = \"us-central1-cluster-demo-lt-4bc9a194-bucket\" ## change this \n",
    "\n",
    "\n",
    "GCS_JOB_FILE_1 = f\"gs://{COMPOSER_BUCKET}/data/INGESTION/hospitalA_mysqlToLanding.py\"\n",
    "PYSPARK_JOB_1 = {\n",
    "    \"reference\": {\"project_id\": PROJECT_ID},\n",
    "    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n",
    "    \"pyspark_job\": {\"main_python_file_uri\": GCS_JOB_FILE_1},\n",
    "}\n",
    "\n",
    "GCS_JOB_FILE_2 = f\"gs://{COMPOSER_BUCKET}/data/INGESTION/hospitalB_mysqlToLanding.py\"\n",
    "PYSPARK_JOB_2 = {\n",
    "    \"reference\": {\"project_id\": PROJECT_ID},\n",
    "    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n",
    "    \"pyspark_job\": {\"main_python_file_uri\": GCS_JOB_FILE_2},\n",
    "}\n",
    "\n",
    "GCS_JOB_FILE_3 = f\"gs://{COMPOSER_BUCKET}/data/INGESTION/claims.py\"\n",
    "PYSPARK_JOB_3 = {\n",
    "    \"reference\": {\"project_id\": PROJECT_ID},\n",
    "    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n",
    "    \"pyspark_job\": {\"main_python_file_uri\": GCS_JOB_FILE_3},\n",
    "}\n",
    "\n",
    "GCS_JOB_FILE_4 = f\"gs://{COMPOSER_BUCKET}/data/INGESTION/cpt_codes.py\"\n",
    "PYSPARK_JOB_4 = {\n",
    "    \"reference\": {\"project_id\": PROJECT_ID},\n",
    "    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n",
    "    \"pyspark_job\": {\"main_python_file_uri\": GCS_JOB_FILE_4},\n",
    "}\n",
    "\n",
    "\n",
    "ARGS = {\n",
    "    \"owner\": \"SHAIK SAIDHUL\",\n",
    "    \"start_date\": None,\n",
    "    \"depends_on_past\": False,\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"email\": [\"***@gmail.com\"],\n",
    "    \"email_on_success\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "# define the dag\n",
    "with DAG(\n",
    "    dag_id=\"pyspark_dag\",\n",
    "    schedule_interval=None,\n",
    "    description=\"DAG to start a Dataproc cluster, run PySpark jobs, and stop the cluster\",\n",
    "    default_args=ARGS,\n",
    "    tags=[\"pyspark\", \"dataproc\", \"etl\", \"marvel\"]\n",
    ") as dag:\n",
    "    \n",
    "    # define the Tasks\n",
    "    start_cluster = DataprocStartClusterOperator(\n",
    "        task_id=\"start_cluster\",\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        cluster_name=CLUSTER_NAME,\n",
    "    )\n",
    "\n",
    "    pyspark_task_1 = DataprocSubmitJobOperator(\n",
    "        task_id=\"pyspark_task_1\", \n",
    "        job=PYSPARK_JOB_1, \n",
    "        region=REGION, \n",
    "        project_id=PROJECT_ID\n",
    "    )\n",
    "\n",
    "    pyspark_task_2 = DataprocSubmitJobOperator(\n",
    "        task_id=\"pyspark_task_2\", \n",
    "        job=PYSPARK_JOB_2, \n",
    "        region=REGION, \n",
    "        project_id=PROJECT_ID\n",
    "    )\n",
    "\n",
    "    pyspark_task_3 = DataprocSubmitJobOperator(\n",
    "        task_id=\"pyspark_task_3\", \n",
    "        job=PYSPARK_JOB_3, \n",
    "        region=REGION, \n",
    "        project_id=PROJECT_ID\n",
    "    )\n",
    "\n",
    "    pyspark_task_4 = DataprocSubmitJobOperator(\n",
    "        task_id=\"pyspark_task_4\", \n",
    "        job=PYSPARK_JOB_4, \n",
    "        region=REGION, \n",
    "        project_id=PROJECT_ID\n",
    "    )\n",
    "\n",
    "    stop_cluster = DataprocStopClusterOperator(\n",
    "        task_id=\"stop_cluster\",\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        cluster_name=CLUSTER_NAME,\n",
    "    )\n",
    "\n",
    "# define the task dependencies\n",
    "start_cluster >> pyspark_task_1 >> pyspark_task_2 >> pyspark_task_3 >> pyspark_task_4 >> stop_cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
